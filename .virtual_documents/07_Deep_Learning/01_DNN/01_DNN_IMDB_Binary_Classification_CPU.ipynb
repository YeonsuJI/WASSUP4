


import warnings
warnings.filterwarnings('ignore')





import tensorflow as tf

tf.__version__








from tensorflow.keras.datasets import imdb

(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = 10000)








import matplotlib.pyplot as plt

print('리뷰 최대 길이 :', max(len(L) for L in X_train))
print('리뷰 평균 길이 :', sum(map(len, X_train))/len(X_train))

plt.figure(figsize = (10, 7))
plt.hist([len(L) for L in X_train], bins = 50)
plt.xlabel('Length of train_data')
plt.ylabel('Number of train_data')
plt.show()





import pandas as pd

DF = pd.DataFrame({'Length':(len(L) for L in X_train[0:10])})

DF


plt.figure(figsize = (10, 7))
DF['Length'].plot(kind = 'barh', 
                  grid = True)
plt.show()





import numpy as np

unique_elements, counts_elements = np.unique(y_train, return_counts = True)

print('Label 빈도수:')
print(np.asarray((unique_elements, counts_elements)))





print('전체 X_train 개수:')
print(len(X_train))

print('첫번째 X_train 정보:')
print(len(X_train[0]))
print(X_train[0])
print(X_train[0].count(2))

print('첫번째 y_train 정보:')
print(y_train[0])








word_index = imdb.get_word_index()

print(word_index)





reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])

print(reverse_word_index)





decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in X_train[0]])

print(decoded_review)
print(y_train[0])





decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in X_train[1]])

print(decoded_review)
print(y_train[1])











import numpy as np

def vectorize_sequences(sequences, dimension = 10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.0
    return results





X_train = vectorize_sequences(X_train)
X_test = vectorize_sequences(X_test)

X_train.shape, X_test.shape





DF2 = pd.DataFrame({'Length':(len(L) for L in X_train[0:10])})

DF2


plt.figure(figsize = (10, 7))
DF2['Length'].plot(kind = 'barh', 
                   grid = True)
plt.show()











from tensorflow.keras import models
from tensorflow.keras import layers

imdb = models.Sequential()
imdb.add(layers.Dense(16, activation = 'relu', input_shape = (10000,)))
imdb.add(layers.Dense(16, activation = 'relu'))
imdb.add(layers.Dense(1, activation = 'sigmoid'))





imdb.summary()








imdb.compile(loss = 'binary_crossentropy',
             optimizer = 'adam',
             metrics = ['accuracy'])





%%time

Hist_imdb = imdb.fit(X_train, y_train,
                     epochs = 50,
                     batch_size = 512,
                     validation_data = (X_test, y_test))





import matplotlib.pyplot as plt

epochs = range(1, len(Hist_imdb.history['loss']) + 1)

plt.figure(figsize = (15, 5))

plt.subplot(1, 2, 1)
plt.plot(epochs, Hist_imdb.history['loss'], 'b-')
plt.plot(epochs, Hist_imdb.history['val_loss'], 'r--')
plt.title('Training & Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend(['Training Loss', 'Validation Loss'])
plt.grid()

plt.subplot(1, 2, 2)
plt.plot(epochs, Hist_imdb.history['accuracy'], 'b-')
plt.plot(epochs, Hist_imdb.history['val_accuracy'], 'r--')
plt.title('Training & Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(['Training Accuracy', 'Validation Accuracy'])
plt.grid()

plt.show()








loss, accuracy = imdb.evaluate(X_test, y_test)

print('Loss = {:.5f}'.format(loss))
print('Accuracy = {:.5f}'.format(accuracy))





np.round(imdb.predict(X_test, verbose = 0))











from tensorflow.keras.preprocessing.sequence import pad_sequences

X_train = pad_sequences(X_train, 
                        maxlen = 2500, 
                        padding = 'post')
X_test = pad_sequences(X_test, 
                       maxlen = 2500, 
                       padding = 'post')





print(X_train[0][:10])
print(X_train[0][-10:])





DF2 = pd.DataFrame({'Length':(len(L) for L in X_train[0:10])})





plt.figure(figsize = (10, 7))
DF2['Length'].plot(kind = 'barh', 
                   grid = True)
plt.show()





print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)











from tensorflow.keras import models
from tensorflow.keras import layers

imdb = models.Sequential()
imdb.add(layers.Dense(16, activation = 'relu', input_shape = (2500,)))
imdb.add(layers.Dense(16, activation = 'relu'))
imdb.add(layers.Dense(1, activation = 'sigmoid'))





imdb.summary()








imdb.compile(loss = 'binary_crossentropy',
             optimizer = 'rmsprop',
             metrics = ['accuracy'])





%%time

Hist_imdb = imdb.fit(X_train, y_train,
                     epochs = 50,
                     batch_size = 512,
                     validation_data = (X_test, y_test))





import matplotlib.pyplot as plt

epochs = range(1, len(Hist_imdb.history['loss']) + 1)

plt.figure(figsize = (15, 5))

plt.subplot(1, 2, 1)
plt.plot(epochs, Hist_imdb.history['loss'], 'b-')
plt.plot(epochs, Hist_imdb.history['val_loss'], 'r--')
plt.title('Training & Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend(['Training Loss', 'Validation Loss'])
plt.grid()

plt.subplot(1, 2, 2)
plt.plot(epochs, Hist_imdb.history['accuracy'], 'b-')
plt.plot(epochs, Hist_imdb.history['val_accuracy'], 'r--')
plt.title('Training & Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(['Training Accuracy', 'Validation Accuracy'])
plt.grid()

plt.show()








loss, accuracy = imdb.evaluate(X_test, y_test)

print('Loss = {:.5f}'.format(loss))
print('Accuracy = {:.5f}'.format(accuracy))





np.round(imdb.predict(X_test, verbose = 0))



